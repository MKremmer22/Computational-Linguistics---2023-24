{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "a function to preprocess text data which can:\n",
        "\n",
        "1.   ◦ remove punctuation\n",
        "2.   ◦ remove stopwords\n",
        "3.   ◦ lowercase all words\n",
        "4.   ◦ remove words below/above a certain frequency"
      ],
      "metadata": {
        "id": "TXPtSz3cV1Ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@Madison Kremmer\n",
        "ID:300523256"
      ],
      "metadata": {
        "id": "-i8q_njRm7WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#working\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "\n",
        "def preprocess_text(text,min_freq=1,max_freq=float('inf')):\n",
        "    # Split pairs into individual words\n",
        "    pairs = text.split()\n",
        "    words = [pair.split('/')[0] for pair in pairs]\n",
        "\n",
        "    # Lowercase conversion\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Remove words consisting only of punctuation or numbers\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_freq = FreqDist(words)\n",
        "\n",
        "    # Remove words based on frequency thresholds\n",
        "    words = [word for word in words if min_freq <= word_freq[word.lower()] <= max_freq]\n",
        "\n",
        "    # Print some information for inspection\n",
        "    print(\"Number of words after preprocessing:\", len(words))\n",
        "    print(\"Words after preprocessing:\", words[:10])\n",
        "    print(\"Word frequencies:\", word_freq)\n",
        "    print('------------------------------------------------------------------')\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNAIxHOM5DLy",
        "outputId": "d572fe04-1f21-46cf-f2b3-68f20ac84aee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a function (or functions) to calculate these text metrics:\n",
        "\n",
        "1.   ◦ total number of words\n",
        "2.   ◦ overall lexical diversity of the text\n",
        "3.   ◦ average lexical diversity of text sentences\n",
        "4.   ◦ top ten most frequent words"
      ],
      "metadata": {
        "id": "nF0bYNsxZdFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def calculate_text_metrics(text):\n",
        "    # Combine sentences into a single string\n",
        "    combined_text = ' '.join(text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(combined_text.lower())\n",
        "\n",
        "    # Calculate total number of words\n",
        "    total_words = len(words)\n",
        "\n",
        "    # Calculate overall lexical diversity of the text\n",
        "    overall_lexical_diversity = len(set(words)) / total_words\n",
        "\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(combined_text)\n",
        "\n",
        "    # Calculate average lexical diversity of text sentences\n",
        "    sentence_lexical_diversities = []\n",
        "    for sentence in sentences:\n",
        "        sentence_words = word_tokenize(sentence.lower())\n",
        "        sentence_words = [word.lower() for word in sentence_words if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "\n",
        "        # Check if the length of sentence_words is greater than zero before division\n",
        "        if len(sentence_words) > 0:\n",
        "            sentence_lexical_diversity = len(set(sentence_words)) / len(sentence_words)\n",
        "            sentence_lexical_diversities.append(sentence_lexical_diversity)\n",
        "\n",
        "    average_sentence_lexical_diversity = (\n",
        "        sum(sentence_lexical_diversities) / len(sentence_lexical_diversities)\n",
        "        if len(sentence_lexical_diversities) > 0\n",
        "        else 0  # Handle the case when there are no sentences\n",
        "    )\n",
        "\n",
        "    # Calculate the frequency distribution of words\n",
        "    word_freq = FreqDist(words)\n",
        "\n",
        "    # Get the top ten most frequent words\n",
        "    top_ten_words = word_freq.most_common(10)\n",
        "\n",
        "    # Display the results\n",
        "    print(\"Total number of words:\", total_words)\n",
        "    print(\"Overall lexical diversity of the text:\", overall_lexical_diversity)\n",
        "    print(\"Average lexical diversity of text sentences:\", average_sentence_lexical_diversity)\n",
        "    print(\"Top ten most frequent words:\", top_ten_words)\n",
        "    print('------------------------------------------------------------------')\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "etuJ1FKOCl2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa67af43-beb2-443b-ab59-19523fed7c08"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this data, look for trends and consistent effects that preprocessing has on various text metrics.\n",
        "Also look to see if there are any texts more or less immune to the effects of preprocessing. After\n",
        "conducting your experiment, write a short report (500-600 words) reflecting on your results.\n",
        "\n",
        "You\n",
        "should detail the comparisons and analyses that you conducted, what results you found, and your\n",
        "interpretation of the results. Specifically, you should focus on what happens to these metrics under\n",
        "different preprocessing conditions, and focus on making conclusions about their implications for text\n",
        "analysis in general."
      ],
      "metadata": {
        "id": "KiuA2xIWZ9yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 1 - Brown\n",
        "\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "document_name = \"ca01\"  # Replace with any document from the corpus\n",
        "text = brown.raw(document_name)\n",
        "\n",
        "# Using the function for the 'brown' corpus format\n",
        "result_brown = preprocess_text(text)\n",
        "print(result_brown)\n",
        "\n",
        "calculate_text_metrics(result_brown)\n",
        "\n",
        "#only words that appear at least 10 times and less than 20\n",
        "print(\"Results of words that appear at least 1 times and less than 4:\")\n",
        "result_brown_10_20 = preprocess_text(text,min_freq = 1, max_freq = 4)\n",
        "calculate_text_metrics(result_brown_10_20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8syBjdmbH_2",
        "outputId": "1bec1bd5-0047-4aa3-b806-c396f8d59162"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words after preprocessing: 1111\n",
            "Words after preprocessing: ['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'recent', 'primary', 'election']\n",
            "Word frequencies: <FreqDist with 657 samples and 1111 outcomes>\n",
            "------------------------------------------------------------------\n",
            "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place', 'jury', 'said', 'presentments', 'city', 'executive', 'committee', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'primary', 'ivan', 'allen', 'relative', 'handful', 'reports', 'received', 'jury', 'said', 'considering', 'widespread', 'interest', 'election', 'number', 'voters', 'size', 'city', 'jury', 'said', 'find', 'many', 'registration', 'election', 'laws', 'outmoded', 'inadequate', 'often', 'ambiguous', 'recommended', 'fulton', 'legislators', 'act', 'laws', 'studied', 'revised', 'end', 'modernizing', 'improving', 'grand', 'jury', 'commented', 'number', 'topics', 'among', 'atlanta', 'fulton', 'county', 'purchasing', 'departments', 'said', 'well', 'operated', 'follow', 'generally', 'accepted', 'practices', 'inure', 'best', 'interest', 'governments', 'merger', 'proposed', 'however', 'jury', 'said', 'believes', 'two', 'offices', 'combined', 'achieve', 'greater', 'efficiency', 'reduce', 'cost', 'administration', 'city', 'purchasing', 'department', 'jury', 'said', 'lacking', 'experienced', 'clerical', 'personnel', 'result', 'city', 'personnel', 'policies', 'urged', 'city', 'take', 'steps', 'remedy', 'problem', 'implementation', 'automobile', 'title', 'law', 'also', 'recommended', 'outgoing', 'jury', 'urged', 'next', 'legislature', 'provide', 'enabling', 'funds', 'effective', 'date', 'orderly', 'implementation', 'law', 'may', 'effected', 'grand', 'jury', 'took', 'swipe', 'state', 'welfare', 'handling', 'federal', 'funds', 'granted', 'child', 'welfare', 'services', 'foster', 'homes', 'one', 'major', 'items', 'fulton', 'county', 'general', 'assistance', 'program', 'jury', 'said', 'state', 'welfare', 'department', 'seen', 'fit', 'distribute', 'funds', 'welfare', 'departments', 'counties', 'state', 'exception', 'fulton', 'county', 'receives', 'none', 'money', 'jurors', 'said', 'realize', 'proportionate', 'distribution', 'funds', 'might', 'disable', 'program', 'less', 'populous', 'counties', 'nevertheless', 'feel', 'future', 'fulton', 'county', 'receive', 'portion', 'available', 'funds', 'jurors', 'said', 'failure', 'continue', 'place', 'disproportionate', 'burden', 'fulton', 'taxpayers', 'jury', 'also', 'commented', 'fulton', 'court', 'fire', 'practices', 'appointment', 'appraisers', 'guardians', 'administrators', 'awarding', 'fees', 'compensation', 'wards', 'protected', 'jury', 'said', 'found', 'court', 'incorporated', 'operating', 'procedures', 'recommendations', 'two', 'previous', 'grand', 'juries', 'atlanta', 'bar', 'association', 'interim', 'citizens', 'committee', 'actions', 'serve', 'protect', 'fact', 'effect', 'wards', 'undue', 'costs', 'appointed', 'elected', 'servants', 'unmeritorious', 'criticisms', 'jury', 'said', 'regarding', 'new', 'airport', 'jury', 'recommended', 'new', 'management', 'takes', 'charge', '1', 'airport', 'operated', 'manner', 'eliminate', 'political', 'influences', 'jury', 'elaborate', 'added', 'periodic', 'surveillance', 'pricing', 'practices', 'concessionaires', 'purpose', 'keeping', 'prices', 'reasonable', 'ask', 'jail', 'deputies', 'matters', 'jury', 'recommended', '1', 'four', 'additional', 'deputies', 'employed', 'fulton', 'county', 'jail', 'doctor', 'medical', 'intern', 'extern', 'employed', 'night', 'weekend', 'duty', 'jail', '2', 'fulton', 'legislators', 'work', 'city', 'officials', 'pass', 'enabling', 'legislation', 'permit', 'establishment', 'fair', 'equitable', 'pension', 'plan', 'city', 'employes', 'jury', 'praised', 'administration', 'operation', 'atlanta', 'police', 'department', 'fulton', 'tax', 'office', 'bellwood', 'alpharetta', 'prison', 'farms', 'grady', 'hospital', 'fulton', 'health', 'department', 'mayor', 'william', 'hartsfield', 'filed', 'suit', 'divorce', 'wife', 'pearl', 'williams', 'hartsfield', 'fulton', 'superior', 'court', 'friday', 'petition', 'charged', 'mental', 'cruelty', 'couple', 'married', '2', '1913', 'son', 'william', 'berry', 'daughter', 'cheshire', 'griffin', 'attorneys', 'mayor', 'said', 'amicable', 'property', 'settlement', 'agreed', 'upon', 'petition', 'listed', 'occupation', 'attorney', 'age', '71', 'listed', 'age', '74', 'place', 'birth', 'opelika', 'petition', 'said', 'couple', 'lived', 'together', 'man', 'wife', 'year', 'hartsfield', 'home', '637', 'pelham', 'aj', 'henry', 'bowden', 'listed', 'petition', 'attorney', 'hartsfield', 'mayor', 'atlanta', 'exception', 'one', 'brief', 'interlude', 'since', '1937', 'political', 'career', 'goes', 'back', 'election', 'city', 'council', '1923', 'present', 'term', 'office', 'expires', '1', 'succeeded', 'ivan', 'allen', 'became', 'candidate', '13', 'primary', 'mayor', 'hartsfield', 'announced', 'would', 'run', 'reelection', 'georgia', 'republicans', 'getting', 'strong', 'encouragement', 'enter', 'candidate', '1962', 'race', 'top', 'official', 'said', 'wednesday', 'robert', 'snodgrass', 'state', 'gop', 'chairman', 'said', 'meeting', 'held', 'tuesday', 'night', 'blue', 'ridge', 'brought', 'enthusiastic', 'responses', 'audience', 'state', 'party', 'chairman', 'james', 'dorsey', 'added', 'enthusiasm', 'picking', 'state', 'rally', 'held', '8', 'savannah', 'newly', 'elected', 'texas', 'john', 'tower', 'featured', 'speaker', 'blue', 'ridge', 'meeting', 'audience', 'warned', 'entering', 'candidate', 'governor', 'would', 'force', 'take', 'petitions', 'voting', 'precincts', 'obtain', 'signatures', 'registered', 'voters', 'despite', 'warning', 'unanimous', 'vote', 'enter', 'candidate', 'according', 'republicans', 'attended', 'crowd', 'asked', 'whether', 'wanted', 'wait', 'one', 'term', 'make', 'race', 'voted', 'dissents', 'largest', 'hurdle', 'republicans', 'would', 'face', 'state', 'law', 'says', 'making', 'first', 'race', 'one', 'two', 'alternative', 'courses', 'must', 'taken', '1', 'five', 'per', 'cent', 'voters', 'county', 'must', 'sign', 'petitions', 'requesting', 'republicans', 'allowed', 'place', 'names', 'candidates', 'general', 'election', 'ballot', '2', 'republicans', 'must', 'hold', 'primary', 'county', 'unit', 'system', 'system', 'party', 'opposes', 'platform', 'sam', 'caldwell', 'state', 'highway', 'department', 'public', 'relations', 'director', 'resigned', 'tuesday', 'work', 'garland', 'campaign', 'resignation', 'expected', 'time', 'succeeded', 'rob', 'ledford', 'gainesville', 'assistant', 'three', 'years', 'gubernatorial', 'campaign', 'starts', 'caldwell', 'expected', 'become', 'campaign', 'coordinator', 'byrd', 'georgia', 'legislature', 'wind', '1961', 'session', 'monday', 'head', 'home', 'highway', 'bond', 'money', 'approved', 'follow', 'shortly', 'adjournment', 'monday', 'afternoon', 'senate', 'expected', 'approve', 'study', 'number', 'legislators', 'allotted', 'rural', 'urban', 'areas', 'determine', 'adjustments', 'made', 'vandiver', 'expected', 'make', 'traditional', 'visit', 'chambers', 'work', 'toward', 'adjournment', 'vandiver', 'likely', 'mention', 'million', 'highway', 'bond', 'issue', 'approved', 'earlier', 'session', 'first', 'priority', 'item', 'construction', 'bonds', 'meanwhile', 'learned', 'state', 'highway', 'department', 'near', 'ready', 'issue', 'first', 'million', 'worth', 'highway', 'reconstruction', 'bonds', 'bond', 'issue', 'go', 'state', 'courts', 'friendly', 'test', 'suit', 'test', 'validity', 'act', 'sales', 'begin', 'contracts', 'let', 'repair', 'work', 'heavily', 'traveled', 'highways', 'highway', 'department', 'source', 'said', 'also', 'plan', 'issue', 'million', 'million', 'worth', 'rural', 'roads', 'authority', 'bonds', 'rural', 'road', 'construction', 'work', 'revolving', 'fund', 'department', 'apparently', 'intends', 'make', 'rural', 'roads', 'authority', 'revolving', 'fund', 'new', 'bonds', 'would', 'issued', 'every', 'time', 'portion', 'old', 'ones', 'paid', 'tax', 'authorities', 'vandiver', 'opened', 'race', 'governor', '1958', 'battle', 'legislature', 'issuance', 'million', 'worth', 'additional', 'rural', 'roads', 'bonds', 'proposed', 'marvin', 'griffin', 'highway', 'department', 'source', 'told', 'constitution', 'however', 'vandiver', 'consulted', 'yet', 'plans', 'issue', 'new', 'rural', 'roads', 'bonds', 'schley', 'county', 'pelham', 'offer', 'resolution', 'monday', 'house', 'rescind', 'action', 'friday', 'voting', 'per', 'day', 'increase', 'expense', 'allowances', 'pelham', 'said', 'sunday', 'night', 'research', 'done', 'whether', 'quickie', 'vote', 'increase', 'repealed', 'outright', 'whether', 'notice', 'would', 'first', 'given', 'reconsideration', 'action', 'would', 'sought', 'emphasizing', 'technical', 'details', 'fully', 'worked', 'pelham', 'said', 'resolution', 'would', 'seek', 'set', 'aside', 'privilege', 'resolution', 'house', 'voted', 'similar', 'resolution', 'passed', 'senate', 'vote', 'sunday', 'night', 'word', 'resolution', 'offered', 'rescind', 'action', 'pelham', 'pointed', 'georgia', 'voters', 'last', 'november', 'rejected', 'constitutional', 'amendment', 'allow', 'legislators', 'vote', 'pay', 'raises', 'future', 'legislature', 'sessions', 'veteran', 'jackson', 'county', 'legislator', 'ask', 'georgia', 'house', 'monday', 'back', 'federal', 'aid', 'education', 'something', 'consistently', 'opposed', 'past', 'mac', 'barber', 'commerce', 'asking', 'house', 'privilege', 'resolution', 'endorse', 'increased', 'federal', 'support', 'public', 'education', 'provided', 'funds', 'received', 'expended', 'state', 'funds', 'barber', '13th', 'year', 'legislator', 'said', 'members', 'congressional', 'delegation', 'washington', 'would', 'like', 'see', 'resolution', 'passed', 'added', 'none', 'congressmen', 'specifically', 'asked', 'offer', 'resolution', 'resolution', 'barber', 'tossed', 'house', 'hopper', 'friday', 'formally', 'read', 'monday', 'says', 'event', 'congress', 'provide', 'increase', 'federal', 'funds', 'state', 'board', 'education', 'directed', 'give', 'priority', 'teacher', 'pay', 'raises', 'colquitt', 'long', 'hot', 'controversy', 'miller', 'county', 'new', 'school', 'superintendent', 'elected', 'policeman', 'put', 'coolest', 'election', 'ever', 'saw', 'county', 'new', 'school', 'superintendent', 'harry', 'davis', 'veteran', 'agriculture', 'teacher', 'defeated', 'felix', 'bush', 'school', 'principal', 'chairman', 'miller', 'county', 'democratic', 'executive', 'committee', 'davis', 'received', 'votes', 'election', 'bush', 'got', '402', 'ordinary', 'carey', 'williams', 'armed', 'pistol', 'stood', 'polls', 'insure', 'order', 'coolest', 'calmest', 'election', 'ever', 'saw', 'colquitt', 'policeman', 'tom', 'williams', 'said', 'polls', 'like', 'church', 'smell', 'drop', 'liquor', 'bit', 'trouble', 'campaign', 'leading', 'election', 'quiet', 'however', 'marked', 'controversy', 'anonymous', 'midnight', 'phone', 'calls', 'veiled', 'threats', 'violence', 'former', 'county', 'school', 'superintendent', 'george', 'callan', 'shot', 'death', 'march', '18', 'four', 'days', 'resigned', 'post', 'dispute', 'county', 'school', 'board', 'election', 'campaign', 'candidates', 'davis', 'bush', 'reportedly', 'received', 'anonymous', 'telephone', 'calls', 'ordinary', 'williams', 'said', 'subjected', 'anonymous', 'calls', 'soon', 'scheduled', 'election', 'many', 'local', 'citizens', 'feared', 'would', 'irregularities', 'polls', 'williams', 'got', 'permit', 'carry', 'gun', 'promised', 'orderly', 'election', 'sheriff', 'felix', 'tabb', 'said', 'ordinary', 'apparently', 'made', 'good', 'promise', 'everything', 'went', 'real', 'smooth', 'sheriff', 'said', 'bit', 'trouble']\n",
            "Total number of words: 1111\n",
            "Overall lexical diversity of the text: 0.5913591359135913\n",
            "Average lexical diversity of text sentences: 0.5880733944954128\n",
            "Top ten most frequent words: [('said', 24), ('jury', 18), ('county', 15), ('fulton', 14), ('election', 14), ('state', 12), ('city', 9), ('department', 9), ('would', 9), ('resolution', 9)]\n",
            "------------------------------------------------------------------\n",
            "Results of words that appear at least 1 times and less than 4:\n",
            "Number of words after preprocessing: 885\n",
            "Words after preprocessing: ['grand', 'friday', 'investigation', 'recent', 'primary', 'produced', 'evidence', 'irregularities', 'took', 'place']\n",
            "Word frequencies: <FreqDist with 657 samples and 1111 outcomes>\n",
            "------------------------------------------------------------------\n",
            "Total number of words: 885\n",
            "Overall lexical diversity of the text: 0.711864406779661\n",
            "Average lexical diversity of text sentences: 0.7106481481481481\n",
            "Top ten most frequent words: [('grand', 4), ('friday', 4), ('primary', 4), ('place', 4), ('court', 4), ('received', 4), ('voters', 4), ('recommended', 4), ('legislators', 4), ('legislature', 4)]\n",
            "------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 2 - The Current Topic Question: Petrol cars should be banned by 2030.\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp001.txt'\n",
        "\n",
        "\n",
        "tp001 = open('tp001.txt').read().rstrip().split('\\n')\n",
        "tp001_comments = [comment.split('\\t')[1] for comment in tp001]\n",
        "tp001_comments[:2]\n",
        "len(tp001_comments)\n",
        "tp001_combined = ' '.join([comment for comment in tp001_comments])\n",
        "tp001_tokens = nltk.word_tokenize(tp001_combined)\n",
        "\n",
        "result_tp001 = preprocess_text(tp001_combined)\n",
        "calculate_text_metrics(result_tp001)\n",
        "\n",
        "print('==Unprocessed==')\n",
        "calculate_text_metrics(tp001_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPB0YKYzbWqQ",
        "outputId": "3b33fd0a-0786-400d-feb6-b45517d88049"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 21:35:04--  https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp001.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 220746 (216K) [text/plain]\n",
            "Saving to: ‘tp001.txt.3’\n",
            "\n",
            "\rtp001.txt.3           0%[                    ]       0  --.-KB/s               \rtp001.txt.3         100%[===================>] 215.57K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-11-23 21:35:04 (5.45 MB/s) - ‘tp001.txt.3’ saved [220746/220746]\n",
            "\n",
            "Number of words after preprocessing: 18987\n",
            "Words after preprocessing: ['need', 'work', 'hard', 'make', 'happen', '3d', 'better', 'bands', 'whole', 'country']\n",
            "Word frequencies: <FreqDist with 4286 samples and 18987 outcomes>\n",
            "------------------------------------------------------------------\n",
            "Total number of words: 19015\n",
            "Overall lexical diversity of the text: 0.22550617933210623\n",
            "Average lexical diversity of text sentences: 0.22432575356953993\n",
            "Top ten most frequent words: [('cars', 492), ('petrol', 362), ('need', 326), ('think', 274), ('electric', 217), ('better', 202), ('good', 200), ('change', 181), ('would', 176), ('planet', 174)]\n",
            "------------------------------------------------------------------\n",
            "==Unprocessed==\n",
            "Total number of words: 41097\n",
            "Overall lexical diversity of the text: 0.11745382874662384\n",
            "Average lexical diversity of text sentences: 0.8857982020583514\n",
            "Top ten most frequent words: [('the', 1507), ('to', 1498), ('.', 1245), ('we', 981), ('and', 968), ('i', 746), ('it', 728), ('a', 708), ('is', 687), ('be', 662)]\n",
            "------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 3 - The Current Topic Question: Nature helps us get through lockdowns\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp008.txt'\n",
        "\n",
        "tp008 = open('tp008.txt').read().rstrip().split('\\n')\n",
        "tp008_comments = [comment.split('\\t')[1] for comment in tp008]\n",
        "tp008_comments[:2]\n",
        "len(tp008_comments)\n",
        "tp008_combined = ' '.join([comment for comment in tp008_comments])\n",
        "tp008_tokens = nltk.word_tokenize(tp008_combined)\n",
        "\n",
        "result_tp008 = preprocess_text(tp008_combined)\n",
        "calculate_text_metrics(result_tp008)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFQJzEYiGXH3",
        "outputId": "ebf30e45-5ae8-412f-f0fe-e6b5d1025480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 01:18:34--  https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp008.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150802 (147K) [text/plain]\n",
            "Saving to: ‘tp008.txt.3’\n",
            "\n",
            "tp008.txt.3         100%[===================>] 147.27K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-11-23 01:18:34 (1.72 MB/s) - ‘tp008.txt.3’ saved [150802/150802]\n",
            "\n",
            "Number of words after preprocessing: 13276\n",
            "Words after preprocessing: ['santana', 'stink', 'bum', 'would', 'good', 'coing', 'stress', 'reconnect', 'really', 'nature']\n",
            "Word frequencies: <FreqDist with 3631 samples and 13276 outcomes>\n",
            "------------------------------------------------------------------\n",
            "Total number of words: 13284\n",
            "Overall lexical diversity of the text: 0.27341162300511895\n",
            "Average lexical diversity of text sentences: 0.2720888083371092\n",
            "Top ten most frequent words: [('nature', 621), ('think', 359), ('good', 240), ('would', 212), ('idea', 212), ('people', 190), ('need', 184), ('us', 180), ('great', 160), ('time', 143)]\n",
            "------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 4 - The Great Gatsby\n",
        "\n",
        "import requests\n",
        "r = requests.get(r'https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
        "great_gatsby = r.text\n",
        "\n",
        "#subset for the book text\n",
        "#(removing the project gutenburg introduction/footnotes)\n",
        "great_gatsby = great_gatsby[1433:277912]\n",
        "#print(great_gatsby)\n",
        "\n",
        "result_gatsby = preprocess_text(great_gatsby)\n",
        "calculate_text_metrics(result_gatsby)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVo0oBHGGdgn",
        "outputId": "9877cc61-dbf6-45af-cb7b-2efc0f424081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words after preprocessing: 16803\n",
            "Words after preprocessing: ['people', 'world', 'advantages', 'say', 'always', 'unusually', 'communicative', 'reserved', 'understood', 'meant']\n",
            "Word frequencies: <FreqDist with 4725 samples and 16803 outcomes>\n",
            "------------------------------------------------------------------\n",
            "Total number of words: 16806\n",
            "Overall lexical diversity of the text: 0.28126859454956565\n",
            "Average lexical diversity of text sentences: 0.281183051654368\n",
            "Top ten most frequent words: [('said', 164), ('one', 130), ('like', 115), ('tom', 115), ('gatsby', 105), ('came', 103), ('daisy', 99), ('little', 91), ('went', 90), ('back', 89)]\n",
            "------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "r = requests.get(r'https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
        "great_gatsby = r.text\n",
        "\n",
        "#subset for the book text\n",
        "#(removing the project gutenburg introduction/footnotes)\n",
        "great_gatsby = great_gatsby[1433:277912]\n",
        "#print(great_gatsby)\n",
        "\n",
        "result_gatsby = preprocess_text(great_gatsby)\n",
        "Metrics_gatsby = calculate_text_metrics(result_gatsby)\n",
        "print(Metrics_gatsby)\n",
        "\n",
        "print(\"Results of words that appear at least 1 times and less than 4:\")\n",
        "gatsby_1_4 = preprocess_text(great_gatsby,min_freq = 1, max_freq = 4)\n",
        "calculate_text_metrics(gatsby_1_4)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9BHrLaTvlyc",
        "outputId": "e2700797-68bb-4de9-c674-95b15ba5c7a6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words after preprocessing: 16803\n",
            "Words after preprocessing: ['people', 'world', 'advantages', 'say', 'always', 'unusually', 'communicative', 'reserved', 'understood', 'meant']\n",
            "Word frequencies: <FreqDist with 4725 samples and 16803 outcomes>\n",
            "------------------------------------------------------------------\n",
            "Total number of words: 16806\n",
            "Overall lexical diversity of the text: 0.28126859454956565\n",
            "Average lexical diversity of text sentences: 0.281183051654368\n",
            "Top ten most frequent words: [('said', 164), ('one', 130), ('like', 115), ('tom', 115), ('gatsby', 105), ('came', 103), ('daisy', 99), ('little', 91), ('went', 90), ('back', 89)]\n",
            "------------------------------------------------------------------\n",
            "None\n",
            "Results of words that appear at least 1 times and less than 4:\n",
            "Number of words after preprocessing: 6018\n",
            "Words after preprocessing: ['advantages', 'unusually', 'communicative', 'reserved', 'meant', 'deal', 'inclined', 'reserve', 'habit', 'natures']\n",
            "Word frequencies: <FreqDist with 4725 samples and 16803 outcomes>\n",
            "------------------------------------------------------------------\n",
            "Total number of words: 6021\n",
            "Overall lexical diversity of the text: 0.6640093007806013\n",
            "Average lexical diversity of text sentences: 0.6638976574181757\n",
            "Top ten most frequent words: [('meant', 4), ('mind', 4), ('intimate', 4), ('usually', 4), ('uniform', 4), ('attention', 4), ('gives', 4), ('miles', 4), ('dignified', 4), ('likely', 4)]\n",
            "------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "v5hw-qvDYIGT"
      }
    }
  ]
}